data2[i] = data2[i]/counts[i]
}
data2 = data2/sum(data2)
myvar = function(data, counts){
data2 = as.numeric(data[2:6])
for(i in 1:5){
data2[i] = data2[i]/counts[i]
}
data2 = data2/sum(data2)
return(var(data2))
}
myvar_test = apply(tidy_var, 1, myvar, counts = tidy_train_count$nn)
myvar_test
max(myvar_test)
min(myvar_test)
which.max(myvar_test)
tidy_var$word[3758]
tidy_var = tidy_all %>%
group_by(word, stars) %>%
summarise(count = n()) %>%
spread(stars, count, fill = 0)
tidy_var
tidy_var$word
tidy_all
library(tidyverse)
library(tidytext)
library(Matrix)
library(topicmodels)
library(glmnet)
data_ori = read_csv("../../data/random100000w.csv")
data_df = data_frame(line = 1:nrow(data_ori),
text = data_ori$text,
stars = data_ori$stars)
dim(data_df)
tidy_all <- data_df %>%
unnest_tokens(word, text)
count <- tidy_all %>%
count(stars)
tidy_var = tidy_all %>%
group_by(word, stars) %>%
summarise(count = n()) %>%
spread(stars, count, fill = 0)
tidy_all
tidy_var
myvar_test = apply(tidy_var, 1, myvar, counts = tidy_train_count$nn)
myvar = function(data, counts){
data2 = as.numeric(data[2:6])
for(i in 1:5){
data2[i] = data2[i]/counts[i]
}
data2 = data2/sum(data2)
return(var(data2))
}
Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
mypred = function(sh, pred_ori){
pred_new = rep(0, length(pred_ori))
pred_new[which(pred_ori<sh[1])] = 1
pred_new[which(pred_ori>=sh[1] & pred_ori<sh[2])] = 2
pred_new[which(pred_ori>=sh[2] & pred_ori<sh[3])] = 3
pred_new[which(pred_ori>=sh[3] & pred_ori<sh[4])] = 4
pred_new[which(pred_ori>=sh[4])] = 5
return(pred_new)
}
add_not = function(x){
return(paste("not", x[1], sep = ""))
}
myvar_test = apply(tidy_var, 1, myvar, counts = tidy_train_count$nn)
tidy_var
data = tidy_var[1,]
count
myvar_test = apply(tidy_var, 1, myvar, counts = count$n)
var_not <- data_frame(word = tidy_var$word, var = myvar_test)
var_not
var_sh
var_sh = 0.001
index_not = which(var_not$var>var_sh)
index_not
tidy_all
stop_words
tidy_var = tidy_all %>%
filter(word %in% stop_words$word)
tidy_var = tidy_all %>%
filter(word %in% stop_words$word) %>%
group_by(word, stars) %>%
summarise(count = n()) %>%
spread(stars, count, fill = 0)
tidy_var
myvar_test = apply(tidy_var, 1, myvar, counts = count$n)
tidy_var$word
length(myvar_test)
var_not <- data_frame(word = tidy_var$word, var = myvar_test)
var_not
index_not = which(var_not$var>var_sh)
my_stop = stop_words[-index_not,]
my_stop
my_stop2 <- my_stop
my_stop2$word = apply(my_stop, 1, add_not)
tidy_all2 <- tidy_all %>%
anti_join(my_stop, by = "word") %>%
anti_join(my_stop2, by = "word")
n_sh = 100
major <- tidy_all2 %>%
count(word, sort = T) %>%
filter(n > n_sh)
major
tidy_all3 <- tidy_all2 %>%
inner_join(major, by = "word")
tidy_all3
tidy_train_count <- tidy_all3 %>%
count(stars)
tidy_train_count
tidy_var = tidy_all3 %>%
group_by(word, stars) %>%
summarise(count = n()) %>%
spread(stars, count, fill = 0)
tidy_var
myvar_test = apply(tidy_var, 1, myvar, counts = tidy_train_count$nn)
word_var <- data_frame(word = tidy_var$word, var = myvar_test)
word_var <- word_var %>%
arrange(desc(var))
word_var <- data_frame(word = tidy_var$word, var = myvar_test) %>%
arrange(desc(var))
word_var
k = sum(word_var$var>var_sh)
k
k = sum(word_var$var>0.01)
k
n_var
var_sh
k = sum(word_var$var>var_sh)
k
var_sh = 0.01
k = sum(word_var$var>var_sh)
k
feature = word_var[1:k,]
write_csv(feature, "../../data/feature100.csv")
myvar = function(data, counts){
data2 = as.numeric(data[2:6])
for(i in 1:5){
data2[i] = data2[i]/counts[i]
}
data2 = data2/sum(data2)
return(var(data2))
}
Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
mypred = function(sh, pred_ori){
pred_new = rep(0, length(pred_ori))
pred_new[which(pred_ori<sh[1])] = 1
pred_new[which(pred_ori>=sh[1] & pred_ori<sh[2])] = 2
pred_new[which(pred_ori>=sh[2] & pred_ori<sh[3])] = 3
pred_new[which(pred_ori>=sh[3] & pred_ori<sh[4])] = 4
pred_new[which(pred_ori>=sh[4])] = 5
return(pred_new)
}
add_not = function(x){
return(paste("not", x[1], sep = ""))
}
train_ori = read_csv("../../data/random100000w.csv")
test_ori = read_csv("/Users/Lyf/OneDrive/study/WISC/2017_spring/Stat_628/hw2/test.csv")
train = data_frame(line = 1:nrow(train_ori),
text = train_ori$text,
stars = train_ori$stars)
test = data_frame(line = test_ori$Id,
text = test_ori$text,
stars = 0)
n1 = nrow(test)
features = read_csv("../../data/feature100.csv")
major <- features %>%
select(word)
major
# my_sh = c(1.45, 2.92, 3.6, 4.15)
my_sh = c(1.314392, 3.028014, 3.541586, 4.148861)
t1 = Sys.time()
tidy_train <- train %>%
unnest_tokens(word, text)
t2 = Sys.time()
print(t2-t1)
t2 = Sys.time()
tidy_test <- test %>%
unnest_tokens(word, text)
t3 = Sys.time()
print(t3-t2)
tidy_train2 <- tidy_train %>%
inner_join(major, by = "word") %>%
mutate(count = 1)
xx <- tidy_train2 %>%
group_by(line) %>%
summarise(star = mean(stars))
w = which(!train$line   %in% xx$line)
w
tidy_train2$stars[w]
length(w)
length(w)/nrow(train)
var_sh = 0.001
dim(tidy_all)
tidy_all <- data_df %>%
unnest_tokens(word, text)
dim(data_df)
count <- tidy_all %>%
count(stars)
tidy_var = tidy_all %>%
filter(word %in% stop_words$word) %>%
group_by(word, stars) %>%
summarise(count = n()) %>%
spread(stars, count, fill = 0)
myvar_test = apply(tidy_var, 1, myvar, counts = count$n)
var_not <- data_frame(word = tidy_var$word, var = myvar_test)
index_not = which(var_not$var>var_sh)
my_stop = stop_words[-index_not,]
my_stop2 <- my_stop
my_stop2$word = apply(my_stop, 1, add_not)
tidy_all2 <- tidy_all %>%
anti_join(my_stop, by = "word") %>%
anti_join(my_stop2, by = "word")
n_sh = 100
major <- tidy_all2 %>%
count(word, sort = T) %>%
filter(n > n_sh)
tidy_all3 <- tidy_all2 %>%
inner_join(major, by = "word")
tidy_train_count <- tidy_all3 %>%
count(stars)
tidy_var = tidy_all3 %>%
group_by(word, stars) %>%
summarise(count = n()) %>%
spread(stars, count, fill = 0)
myvar_test = apply(tidy_var, 1, myvar, counts = tidy_train_count$nn)
word_var <- data_frame(word = tidy_var$word, var = myvar_test) %>%
arrange(desc(var))
k = sum(word_var$var>var_sh)
k
k = sum(word_var$var>var_sh)
feature = word_var[1:k,]
feature
write_csv(feature, "../../data/feature100.csv")
features = read_csv("../../data/feature100.csv")
major <- features %>%
select(word)
major
tidy_train2 <- tidy_train %>%
inner_join(major, by = "word") %>%
mutate(count = 1)
xx <- tidy_train2 %>%
group_by(line) %>%
summarise(star = mean(stars))
w = which(!train$line   %in% xx$line)
w
star_mode = round(mean(train$stars))
my_tidy = tibble(
line = w,
stars = rep(star_mode, length(w)),
word = rep("myWord", length(w)),
count = rep(1, length(w))
)
tidy_train3 <- full_join(tidy_train2, my_tidy, by = c("line", "stars", "word", "count")) %>%
arrange(line)
tidy_test2 <- tidy_test %>%
inner_join(major, by = "word") %>%
mutate(count = 1)
xx <- tidy_test2 %>%
group_by(line) %>%
summarise(star = mean(stars))
w = which(!test$line   %in% xx$line)
w
my_tidy = tibble(
line = w,
stars = rep(star_mode, length(w)),
word = rep("myWord", length(w)),
count = rep(1, length(w))
)
t2 = Sys.time()
tidy_test3 <- full_join(tidy_test2, my_tidy, by = c("line", "stars", "word", "count")) %>%
arrange(line)
t3 = Sys.time()
print(t3-t2)
tidy_train3$line = tidy_train3$line + n1
tidy_train4 <- tidy_train3 %>%
bind_tf_idf(word, line, count)
tidy_test4 <- tidy_test3 %>%
bind_tf_idf(word, line, count)
write_csv(tidy_train4, "/Users/Lyf/Desktop/tidy_train_tfidf_n100.csv")
write_csv(tidy_test4, "/Users/Lyf/Desktop/tidy_test_tfidf_n100.csv")
dim(tidy_train4)
dim(tidy_test4)
tidy_all <- bind_rows(tidy_test4, tidy_train4)
tidy_all
t2 = Sys.time()
all_matrix <- tidy_all %>%
cast_sparse(line, word, tf_idf)
t3 = Sys.time()
print(t3-t2)
starss = c(rep(0, n1), train$stars)
n2 = dim(all_matrix)[2]
all_matrix2 = cbind(all_matrix, starss)
test_index = c(1:n1)
train_matrix = all_matrix2[-test_index,]
test_matrix = all_matrix2[test_index,]
fit = glmnet(train_matrix[,1:n2], train_matrix[,n2+1])
cv <- cv.glmnet(train_matrix[,1:n2], train_matrix[,n2+1], nfolds=5)
pred <- predict(fit, test_matrix[, 1:n2], type="response", s=cv$lambda.min)
my_sh = c(1.573427, 2.900370, 3.400423, 4.290100)
pred_new = mypred(my_sh, pred)
pred_new
result = tibble(Id = c(1:length(pred_new)),
Prediction1 = pred_new)
write_csv(result, "/Users/Lyf/Desktop/result100.csv")
var_sh = 0.001
tidy_all <- data_df %>%
unnest_tokens(word, text)
count <- tidy_all %>%
count(stars)
tidy_var = tidy_all %>%
filter(word %in% stop_words$word) %>%
group_by(word, stars) %>%
summarise(count = n()) %>%
spread(stars, count, fill = 0)
tidy_var
tidy_var = tidy_all %>%
# filter(word %in% stop_words$word) %>%
group_by(word, stars) %>%
summarise(count = n()) %>%
spread(stars, count, fill = 0)
tidy_var
which(tidy_all$word=="__ ")
tidy_var$word
tidy_var$word[1:20]
tidy_var
tidy_all
testtt <- tidy_all %>%
# filter(word %in% stop_words$word) %>%
group_by(word, stars) %>%
summarise(count = n())
testtt
data_ori = read_csv("../../data/random100000w.csv")
data_df = data_frame(line = 1:nrow(data_ori),
text = data_ori$text,
stars = data_ori$stars)
var_sh = 0.001
tidy_all <- data_df %>%
unnest_tokens(word, text)
count <- tidy_all %>%
count(stars)
tidy_var = tidy_all %>%
# filter(word %in% stop_words$word) %>%
group_by(word, stars) %>%
summarise(count = n()) %>%
spread(stars, count, fill = 0)
tidy_var
tidy_all$word[1:10]
?gsub
gsub(pattern = "[^a-zA-Z]",replacement = "", x = "a12b3c")
tidy_all
word_en = apply(tidy_all[,3], 1, gsub, pattern = "[^a-zA-Z]", replacement = "")
word_en
length(word_en)
dim(tidy_all)
tidy_all$word = word_en
count <- tidy_all %>%
count(stars)
tidy_var = tidy_all %>%
# filter(word %in% stop_words$word) %>%
group_by(word, stars) %>%
summarise(count = n()) %>%
spread(stars, count, fill = 0)
tidy_var
tidy_var[1,]
5+9+5+7+19
tidy_var = tidy_all %>%
filter(word %in% stop_words$word) %>%
group_by(word, stars) %>%
summarise(count = n()) %>%
spread(stars, count, fill = 0)
myvar_test = apply(tidy_var, 1, myvar, counts = count$n)
var_not <- data_frame(word = tidy_var$word, var = myvar_test)
var_not
index_not = which(var_not$var>var_sh)
my_stop = stop_words[-index_not,]
my_stop2 <- my_stop
my_stop2$word = apply(my_stop, 1, add_not)
tidy_all2 <- tidy_all %>%
anti_join(my_stop, by = "word") %>%
anti_join(my_stop2, by = "word")
n_sh = 100
major <- tidy_all2 %>%
count(word, sort = T) %>%
filter(n > n_sh)
major
tidy_all3 <- tidy_all2 %>%
inner_join(major, by = "word")
tidy_train_count <- tidy_all3 %>%
count(stars)
tidy_var = tidy_all3 %>%
group_by(word, stars) %>%
summarise(count = n()) %>%
spread(stars, count, fill = 0)
myvar_test = apply(tidy_var, 1, myvar, counts = tidy_train_count$nn)
word_var <- data_frame(word = tidy_var$word, var = myvar_test) %>%
arrange(desc(var))
word_var
k = sum(word_var$var>var_sh)
k
nrow(word_var)
k = sum(word_var$var>var_sh)
feature = word_var[1:k,]
write_csv(feature, "../../data/feature100.csv")
train_ori = read_csv("../../data/random100000w.csv")
test_ori = read_csv("/Users/Lyf/OneDrive/study/WISC/2017_spring/Stat_628/hw2/test.csv")
train = data_frame(line = 1:nrow(train_ori),
text = train_ori$text,
stars = train_ori$stars)
test = data_frame(line = test_ori$Id,
text = test_ori$text,
stars = 0)
n1 = nrow(test)
features = read_csv("../../data/feature100.csv")
major <- features %>%
select(word)
t1 = Sys.time()
tidy_train <- train %>%
unnest_tokens(word, text)
t2 = Sys.time()
print(t2-t1)
tidy_train
t2 = Sys.time()
word_en = apply(tidy_train[,3], 1, gsub, pattern = "[^a-zA-Z]", replacement = "")
tidy_train$word = word_en
print(Sys.time()-t2)
t2 = Sys.time()
tidy_test <- test %>%
unnest_tokens(word, text)
t3 = Sys.time()
print(t3-t2)
tidy_train2 <- tidy_train %>%
inner_join(major, by = "word") %>%
mutate(count = 1)
xx <- tidy_train2 %>%
group_by(line) %>%
summarise(star = mean(stars))
w = which(!train$line   %in% xx$line)
w
star_mode = round(mean(train$stars))
star_mode
my_tidy = tibble(
line = w,
stars = rep(star_mode, length(w)),
word = rep("myWord", length(w)),
count = rep(1, length(w))
)
tidy_trai
tidy_train
test$stars[w]
train$stars[w]
my_tidy = tibble(
line = w,
stars = rep(star_mode, length(w)),
word = rep("myWord", length(w)),
count = rep(1, length(w))
)
tidy_train3 <- full_join(tidy_train2, my_tidy, by = c("line", "stars", "word", "count")) %>%
arrange(line)
tidy_test2 <- tidy_test %>%
inner_join(major, by = "word") %>%
mutate(count = 1)
xx <- tidy_test2 %>%
group_by(line) %>%
summarise(star = mean(stars))
w = which(!test$line   %in% xx$line)
my_tidy = tibble(
line = w,
stars = rep(star_mode, length(w)),
word = rep("myWord", length(w)),
count = rep(1, length(w))
)
t2 = Sys.time()
tidy_test3 <- full_join(tidy_test2, my_tidy, by = c("line", "stars", "word", "count")) %>%
arrange(line)
t3 = Sys.time()
print(t3-t2)
tidy_train3$line
tidy_train3$line = tidy_train3$line + n1
tidy_train4 <- tidy_train3 %>%
bind_tf_idf(word, line, count)
tidy_test4 <- tidy_test3 %>%
bind_tf_idf(word, line, count)
tidy_all <- bind_rows(tidy_test4, tidy_train4)
t2 = Sys.time()
all_matrix <- tidy_all %>%
cast_sparse(line, word, tf_idf)
t3 = Sys.time()
print(t3-t2)
starss = c(rep(0, n1), train$stars)
n2 = dim(all_matrix)[2]
all_matrix2 = cbind(all_matrix, starss)
test_index = c(1:n1)
train_matrix = all_matrix2[-test_index,]
test_matrix = all_matrix2[test_index,]
fit = glmnet(train_matrix[,1:n2], train_matrix[,n2+1])
cv <- cv.glmnet(train_matrix[,1:n2], train_matrix[,n2+1], nfolds=5)
pred <- predict(fit, test_matrix[, 1:n2], type="response", s=cv$lambda.min)
# my_sh = c(1.5, 2.5, 3.5, 4.5)
my_sh = c(1.573427, 2.900370, 3.400423, 4.290100)
pred_new = mypred(my_sh, pred)
result = tibble(Id = c(1:length(pred_new)),
Prediction1 = pred_new)
write_csv(result, "/Users/Lyf/Desktop/result100.csv")
a
