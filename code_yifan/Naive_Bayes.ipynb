{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import enchant\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/li_first100000.csv'\n",
    "reviews = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(x):\n",
    "    return str(x).lower() != 'nan'\n",
    "index = reviews['text'].apply(isNaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10000\n",
    "review2 = reviews.loc[:N, 'text']\n",
    "review2 = review2[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        serious cannot stand thi mcdonald never get or...\n",
       "1        amaz food truli excel best lobster bisqu prime...\n",
       "2        thi wa second time seafood wa awesom rememb go...\n",
       "3        long stori short bunch rude heartless ignor as...\n",
       "4        grab dinner last night befor catch show gammag...\n",
       "5        great place came lunch brodey ashley one femal...\n",
       "6        wife go good egg sinc thi good egg sinc open t...\n",
       "7        huge menu select restaur look great remodel se...\n",
       "8        get close midnight still not_had not_dinner no...\n",
       "9        saw thi restaur whim wa look good indian food ...\n",
       "10       town confer check yelp great local spot eat th...\n",
       "11       came clockwork coffe share space peopl behind ...\n",
       "12       thi local donut shop wa amaz fast friendli foo...\n",
       "13       first time tri happi hour menu awesom great fo...\n",
       "14       place use mayb still jjanga howev sakana ayc s...\n",
       "15       like mani yelper stop mom sister late lunch wa...\n",
       "16       food ambianc alway great beauti bridal shower ...\n",
       "17       food wa good appl arugula salad order wa delic...\n",
       "18       love thi place never go wrong order mula flowe...\n",
       "19       idea food apathet hostess inform us pizza befo...\n",
       "20       blech three us bought gener tao chicken wing m...\n",
       "21       rock bottom ale chicken tonight wa veri larg p...\n",
       "22       love home fri spici dip great white draft also...\n",
       "23       mani fun flavor drive thru alway plu plu oz dr...\n",
       "24       great food wonder place pizza fantast salad wi...\n",
       "25       word disappoint veri unremark steak send back ...\n",
       "26       look clean fresh food check thi place thi visi...\n",
       "27       friend went momofuku dinner dure first open we...\n",
       "28       wasnt fan slow unenthusiast servic kind morbid...\n",
       "29       friendli staff delici food great environ owner...\n",
       "                               ...                        \n",
       "9971     thi place wa nice locat small layout not_cramp...\n",
       "9972     come year margarita fantast food typic mexican...\n",
       "9973     hachi great sushi spot summerlin area locat re...\n",
       "9974     ca not_sport not_the not_name not_gordon not_r...\n",
       "9975     excel servic not_love not_the not_ahi not_bowl...\n",
       "9976     went dure restaur week juli great food great a...\n",
       "9977     love okonomiyaki thi place serv good one not_e...\n",
       "9978     onc screw order sausag egg mcmuffin said wrapp...\n",
       "9979     come thi bp least onc week never disappoint fo...\n",
       "9980     absolut die love place love encor fantast stea...\n",
       "9981     sister got takeout last night wa first time tr...\n",
       "9982     far good time alway happi good food good servi...\n",
       "9983     gabi et jule je taim describ majesti perfectli...\n",
       "9984     thi alway first stop get vega atmospher remind...\n",
       "9985     not_normal not_write not_neg not_review not_ab...\n",
       "9986     twice time ridicul wait time get bill took alm...\n",
       "9987     sometim life compromis like lunch buddi work w...\n",
       "9988     thi place fantast plaza midwood becom hot spot...\n",
       "9989     thi place like way better version circu circu ...\n",
       "9990     thi place right street offic gone co worker lu...\n",
       "9991     veri sad experi went th wed anniversari waiter...\n",
       "9992     thi best mexican food vega believ not_much not...\n",
       "9993     mom went market first time yesterday hear rave...\n",
       "9994     great famili dinner went group dinner belli da...\n",
       "9995     hype real eaten thai quit place nobodi come cl...\n",
       "9996     like chicken although didnt find smoki expect ...\n",
       "9997     wa not_feel not_well not_when not_we not_went ...\n",
       "9998     great eat sushi place la vega true wait time b...\n",
       "9999     alway hunt authent chicken biryani student tas...\n",
       "10000    sweet employe bad servic everyth come small po...\n",
       "Name: text, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(text):    \n",
    "    return [word for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'second', 'time', 'seafood', 'wa', 'awesom', 'rememb', 'got', 'king', 'crab', 'lobster', 'dinner', 'packag', 'come', 'soup', 'king', 'crab', 'cook', 'differ', 'way', 'steam', 'friend', 'dri', 'garlic', 'fri', 'rice', 'hk', 'style', 'lobster', 'lobster', 'tower', 'see', 'photo', 'snow', 'pea', 'dessert', 'thi', 'packag', 'meant', 'peopl', 'pre', 'tax', 'tip', 'howev', 'becaus', 'king', 'crab', 'larger', 'size', 'come', 'thi', 'packag', 'lb', 'pay', 'addit', 'extra', 'weight', 'think', 'amount', 'food', 'got', 'could', 'probabl', 'fed', 'parti', 'love', 'fri', 'lobster', 'crab', 'leg', 'super', 'yummi', 'thi', 'restaur', 'definit', 'worth', 'visit', 'ani', 'seafood', 'lover', 'howev', 'restaur', 'doe', 'get', 'veri', 'busi', 'make', 'sure', 'call', 'ahead', 'make', 'reserv', 'esp', 'plan', 'come', 'dure', 'weekend', 'tabl', 'restaur', 'pack', 'veri', 'tightli', 'round', 'tabl', 'wa', 'wa', 'liter', 'touch', 'tabl', 'besid', 'us', 'not_come', 'not_here', 'not_if', 'not_you', 'not_are', 'not_expect', 'not_a', 'not_restaur', 'not_with', 'not_a', 'not_nice', 'not_ambianc', 'not_or', 'not_a', 'not_place', 'not_where', 'not_you', 'not_want', 'not_to', 'not_talk', 'not_to', 'not_your', 'not_friend', 'not_in', 'not_privat', 'not_becaus', 'not_the', 'not_tabl', 'not_besid', 'not_you', 'not_will', 'not_definit', 'not_hear', 'not_everi', 'not_word', 'not_you', 'not_are', 'not_say']\n"
     ]
    }
   ],
   "source": [
    "X = review2\n",
    "sample_text = X[2]\n",
    "print(text_split(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=text_split).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24572"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_transformer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 686)\t1\n",
      "  (0, 1212)\t1\n",
      "  (0, 1363)\t1\n",
      "  (0, 3006)\t1\n",
      "  (0, 4237)\t1\n",
      "  (0, 5088)\t1\n",
      "  (0, 5849)\t1\n",
      "  (0, 7592)\t1\n",
      "  (0, 7697)\t1\n",
      "  (0, 7785)\t1\n",
      "  (0, 7823)\t1\n",
      "  (0, 8198)\t1\n",
      "  (0, 8313)\t1\n",
      "  (0, 8821)\t1\n",
      "  (0, 9852)\t1\n",
      "  (0, 10030)\t1\n",
      "  (0, 10411)\t2\n",
      "  (0, 11002)\t1\n",
      "  (0, 11372)\t1\n",
      "  (0, 11517)\t1\n",
      "  (0, 11939)\t1\n",
      "  (0, 12730)\t1\n",
      "  (0, 12970)\t1\n",
      "  (0, 13703)\t1\n",
      "  (0, 14088)\t1\n",
      "  :\t:\n",
      "  (0, 15195)\t1\n",
      "  (0, 15523)\t1\n",
      "  (0, 15648)\t1\n",
      "  (0, 16191)\t1\n",
      "  (0, 16411)\t1\n",
      "  (0, 17152)\t1\n",
      "  (0, 17249)\t1\n",
      "  (0, 18139)\t1\n",
      "  (0, 18272)\t1\n",
      "  (0, 18281)\t3\n",
      "  (0, 18297)\t1\n",
      "  (0, 19076)\t1\n",
      "  (0, 19203)\t1\n",
      "  (0, 19293)\t1\n",
      "  (0, 19352)\t1\n",
      "  (0, 21870)\t1\n",
      "  (0, 22640)\t1\n",
      "  (0, 23246)\t1\n",
      "  (0, 23803)\t1\n",
      "  (0, 23958)\t1\n",
      "  (0, 24067)\t1\n",
      "  (0, 24296)\t1\n",
      "  (0, 24355)\t1\n",
      "  (0, 24369)\t1\n",
      "  (0, 24375)\t1\n"
     ]
    }
   ],
   "source": [
    "bow_25 = bow_transformer.transform([review2[25]])\n",
    "print(bow_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8ac5ad8a2095>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbow_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \"\"\"\n\u001b[0;32m--> 836\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-21714d098e8f>\u001b[0m in \u001b[0;36mtext_process\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnopunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnopunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnopunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-21714d098e8f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnopunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnopunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnopunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mline_tokenize\u001b[0;34m(text, blanklines)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblanklines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'discard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLineTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblanklines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;31m# If requested, strip off blank lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
